# model contents
# these models are safetensors
ls ~/.cache/huggingface/hub/models--yentinglin--Taiwan-LLM-7B-v2.0.1-chat/snapshots/0f23bcd591a91419e6a72e5c0a31773512a31788/

# git lfs install, if no git lfs cmd
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
git lfs install

# for safetensors
# https://github.com/jmorganca/ollama/blob/main/docs/import.md#importing-pytorch--safetensors
git clone https://huggingface.co/yentinglin/Taiwan-LLM-7B-v2.0.1-chat
cd Taiwan-LLM-7B-v2.0.1-chat
docker run --rm -v .:/model ollama/quantize -q q4_0 /model

# built bins:
# models/Taiwan-LLM-7B-v2.0.1-chat/q4_0.bin
# models/Taiwan-LLM-7B-v2.0.1-chat/f16.bin

# install NVIDIA Container Toolkit packages for docker ollama
# https://hub.docker.com/r/ollama/ollama
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update

sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Start ollama
docker compose up -d

# Create model
curl -X POST http://localhost:11434/api/create -d '{
    "name": "Taiwan-LLM-13B-v2.0-chat",
    "path": "/root/Modelfile",
    "modelfile": "FROM /root/models/Taiwan-LLM-13B-v2.0-chat/q4_0.bin"
}' | jq

# optional
# "stream": false

# path is deprecated but required, wtf?
# https://github.com/jmorganca/ollama/blob/main/docs/api.md#parameters-1

# List local models
curl http://localhost:11434/api/tags | jq

# Generate (stream)
curl -X POST http://localhost:11434/api/generate -d '{
    "model": "Taiwan-LLM-7B-v2.0.1-chat:latest",
    "prompt": "請幫我用 ffmpeg cli 把兩段 mp4 影片組合成一段影片，並詳細說明 params 的作用",
}'

# Generate (no stream)
curl -X POST http://localhost:11434/api/generate -d '{
    "model": "Taiwan-LLM-7B-v2.0.1-chat:latest",
    "prompt": "請幫我用 ffmpeg cli 把兩段 mp4 影片組合成一段影片，並詳細說明所有參數的作用",
    "stream": false
}' | jq 'del(.context)'
# for time measurement
# -o /dev/null -s -w 'Total: %{time_total}s\n'

# running result, response time ~10s
# GPU status:
```
Fri Nov 17 06:51:46 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3080        Off | 00000000:3B:00.0 Off |                  N/A |
|  0%   61C    P2             177W / 380W |   3451MiB / 10240MiB |     40%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 3080        Off | 00000000:3C:00.0 Off |                  N/A |
|  0%   60C    P2             144W / 380W |   2418MiB / 10240MiB |     29%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

# Delete model
curl -X DELETE http://localhost:11434/api/delete -d '{
    "name": "Taiwan-LLM-7B-v2.0.1-chat:latest"
}' | jq
